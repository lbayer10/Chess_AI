# Chess engine using Deep Reinforcement learning

1. Use MCTS to choose the next move (PUCT = Predictor + Upper Confidence Bound tree search)
2. After a certain depth, use a neural network to evaluate the best moves found by MCTS
3. Select the best move from the neural network
4. Keep doing this 

## Useful sources

* https://en.wikipedia.org/wiki/Deep_reinforcement_learning
* https://en.wikipedia.org/wiki/Reinforcement_learning
* https://en.wikipedia.org/wiki/AlphaZero
* https://en.wikipedia.org/wiki/AlphaGo & https://en.wikipedia.org/wiki/AlphaGo_Zero
* https://en.wikipedia.org/wiki/Monte_Carlo_tree_search
* https://en.wikipedia.org/wiki/Alpha-beta_pruning
* https://medium.com/applied-data-science/alphago-zero-explained-in-one-diagram-365f5abf67e0
* https://github.com/LeelaChessZero/lc0/wiki/Technical-Explanation-of-Leela-Chess-Zero
* 
